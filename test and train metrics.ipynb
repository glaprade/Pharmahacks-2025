{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install gdown\n", "%pip install numpy\n", "%pip install seaborn\n", "%pip install matplotlib\n", "%pip install pandas\n", "%pip install copy"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%# Anything above 2.10 is not supported on the GPU on Windows Native\n", "% pip install \"tensorflow<2.11\" "]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import copy\n", "import gdown"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_df = pd.read_pickle(\"C:/Users/Rewan/Downloads/train_data.pickle\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_df.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DICT_SYMBOLS = {\n", "    'A': 1,\n", "    'T': 2,\n", "    'C': 3,\n", "    'G': 4}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Puzzle:\n", "    def __init__(self, data):\n", "        self.start = list(data[\"start\"])\n", "        self.moves = copy.deepcopy(data.get(\"moves\"))\n", "        self.steps = copy.deepcopy(data.get(\"steps\"))\n", "        self.solution = list(data[\"solution\"])\n", "        self.score = data[\"score\"]\n", "        self.accepted_pair = data[\"accepted_pair\"]\n", "        self.padded_start = self.build_puzzle_to_end(self.start)\n", "        self.padded_solution = self.build_puzzle_to_end(self.solution)\n", "    def build_puzzle_to_end(self, puzzle):\n", "        \"\"\"Pad each row with '-' to match the longest row for visualization.\"\"\"\n", "        max_len = max(len(row) for row in puzzle)\n", "        return [row.ljust(max_len, '-') for row in puzzle]\n", "    def plot_puzzle(self, puzzle, title):\n", "        puzzle = self.build_puzzle_to_end(puzzle)\n", "        num_puzzle = np.array([[DICT_SYMBOLS.get(char, 0) for char in row] for row in puzzle])\n", "        rot_num_puzzle = np.rot90(num_puzzle, 1)\n", "        puzzle_array = np.array([list(row) for row in puzzle])\n", "        rot_labels = np.rot90(puzzle_array, 1)\n", "        plt.figure(figsize=(8, 6))\n", "        sns.heatmap(rot_num_puzzle, annot=rot_labels, fmt=\"\", cmap=\"Pastel1_r\", cbar=False)\n", "        plt.title(title)\n", "        plt.axis(\"off\")\n", "        plt.show()\n", "    def gearbox_score(self, puzzle, bonus=1.15):\n", "        consensus = self.accepted_pair\n", "        score = 0\n", "        for col_ind in range(len(puzzle[0])):\n", "            col_bonus = True\n", "            col_tot = 0\n", "            column_chars = [row[col_ind] for row in puzzle]\n", "            for char in column_chars:\n", "                if char == \"-\":\n", "                    col_bonus = False\n", "                    continue\n", "                if char in consensus[col_ind]:\n", "                    col_tot += 1\n", "                else:\n", "                    col_bonus = False\n", "            column_score = col_tot * bonus if col_bonus else col_tot\n", "            score += column_score\n", "        print(f\"Total Gearbox Score: {score}\")\n", "        return score\n", "    def _apply_step_to_puzzle(self, puzzle, step):\n", "        \"\"\"Apply a single step to the puzzle.\"\"\"\n", "        new_puzzle = puzzle.copy()\n", "        row_index = step[0] - 1\n", "        col_index = step[1]\n", "        if row_index < 0 or row_index >= len(new_puzzle):\n", "            return new_puzzle\n", "        row_str = new_puzzle[row_index]\n", "        if col_index < 0 or col_index > len(row_str):\n", "            return new_puzzle\n", "        new_row = row_str[:col_index] + '-' + row_str[col_index:]\n", "        new_row = new_row[:len(row_str)]\n", "        new_puzzle[row_index] = new_row\n", "        return new_puzzle\n", "    def apply_all_steps(self):\n", "        \"\"\"Apply all steps on a copy of the puzzle and plot states.\"\"\"\n", "        current_puzzle = list(self.start)\n", "        updated_puzzles = []\n", "        scores = []\n", "        for step in self.steps:\n", "            current_puzzle = self._apply_step_to_puzzle(current_puzzle, step)\n", "            padded_current = self.build_puzzle_to_end(current_puzzle)\n", "            score = self.gearbox_score(padded_current)\n", "            updated_puzzles.append(padded_current)\n", "            scores.append(score)\n", "        n_steps = len(updated_puzzles)\n", "        fig, axes = plt.subplots(1, n_steps, figsize=(4 * n_steps, 6))\n", "        if n_steps == 1:\n", "            axes = [axes]\n", "        for idx, (puzzle_state, score) in enumerate(zip(updated_puzzles, scores)):\n", "            num_puzzle = np.array([[DICT_SYMBOLS.get(char, 0) for char in row] for row in puzzle_state])\n", "            rot_num_puzzle = np.rot90(num_puzzle, 1)\n", "            puzzle_array = np.array([list(row) for row in puzzle_state])\n", "            rot_labels = np.rot90(puzzle_array, 1)\n", "            ax = axes[idx]\n", "            sns.heatmap(rot_num_puzzle, annot=rot_labels, fmt=\"\", cmap=\"Pastel1_r\", cbar=False, ax=ax)\n", "            ax.set_title(f\"Step {idx+1}\\nScore: {score}\")\n", "            ax.axis(\"off\")\n", "        plt.tight_layout()\n", "        plt.show()\n", "        return current_puzzle"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Create puzzle"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["puzzle_data = {\n", "    'start': train_df.iloc[-1]['start'],\n", "    'moves': train_df.iloc[-1].get('moves'),\n", "    'steps': train_df.iloc[-1].get('steps'),\n", "    'solution': train_df.iloc[-1]['solution'],\n", "    'score': train_df.iloc[-1]['score'],\n", "    'accepted_pair': train_df.iloc[-1]['accepted_pair']\n", "}\n", "puzzle = Puzzle(puzzle_data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["puzzle.plot_puzzle(puzzle.start, \"Starting Puzzle\")\n", "puzzle.plot_puzzle(puzzle.solution, \"Solution Puzzle\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["updated_puzzle = puzzle.apply_all_steps()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["puzzle.steps"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "y code team error 404"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Install TensorFlow"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["conda install tensorflow"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pip install tensorflow"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pip install sklearn"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import random\n", "from collections import deque\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense\n", "from tensorflow.keras.optimizers import Adam\n", "from tensorflow.keras.preprocessing.sequence import pad_sequences"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load your dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_df = pd.read_pickle(\"C:/Users/Rewan/Downloads/train_data.pickle\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Preprocess the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def preprocess_data(train_df):\n", "    states = train_df['start'].tolist()\n", "    actions = train_df['moves'].tolist()\n", "    rewards = train_df['score'].tolist()\n", "    \n", "    # Encode states (example: one-hot encoding)\n", "    def encode_state(sequence):\n", "        nucleotides = ['A', 'T', 'C', 'G', '-']  # Include all possible characters\n", "        encoding = np.zeros((len(sequence), len(nucleotides)))\n", "        for i, char in enumerate(sequence):\n", "            if char in nucleotides:  # Only encode valid characters\n", "                encoding[i, nucleotides.index(char)] = 1\n", "            else:\n", "                # Handle unknown characters (e.g., skip or map to a default value)\n", "                pass\n", "        return encoding.flatten()  # Flatten to 1D array\n", "    \n", "    # Pad sequences to a fixed length\n", "    max_length = max(len(seq) for seq in states)\n", "    encoded_states = [encode_state(seq) for seq in states]\n", "    encoded_states = pad_sequences(encoded_states, maxlen=max_length, padding='post', dtype='float32')\n", "    \n", "    # Encode actions (example: assign unique integers)\n", "    unique_actions = list(set([move for sublist in actions for move in sublist]))\n", "    action_to_index = {action: i for i, action in enumerate(unique_actions)}\n", "    encoded_actions = [[action_to_index.get(move, -1) for move in sublist] for sublist in actions]\n", "    \n", "    return encoded_states, encoded_actions, rewards"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Preprocess the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["encoded_states, encoded_actions, rewards = preprocess_data(train_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["DQN Hyperparameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["state_size = encoded_states.shape[1]  # Size of the state vector\n", "action_size = len(set([action for sublist in encoded_actions for action in sublist]))  # Number of unique actions\n", "batch_size = 32\n", "episodes = 1000\n", "memory = deque(maxlen=2000)\n", "gamma = 0.95  # Discount factor\n", "epsilon = 1.0  # Exploration rate\n", "epsilon_min = 0.01\n", "epsilon_decay = 0.995\n", "learning_rate = 0.001"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Build the DQN model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(Dense(24, input_dim=state_size, activation='relu'))  # Input shape is (state_size,)\n", "model.add(Dense(24, activation='relu'))\n", "model.add(Dense(action_size, activation='linear'))\n", "model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["DQN Agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DQNAgent:\n", "    def __init__(self, state_size, action_size):\n", "        self.state_size = state_size\n", "        self.action_size = action_size\n", "        self.memory = deque(maxlen=2000)\n", "        self.gamma = gamma\n", "        self.epsilon = epsilon\n", "        self.epsilon_min = epsilon_min\n", "        self.epsilon_decay = epsilon_decay\n", "        self.model = model\n", "    \n", "    def remember(self, state, action, reward, next_state, done):\n", "        self.memory.append((state, action, reward, next_state, done))\n", "    \n", "    def act(self, state):\n", "        if np.random.rand() <= self.epsilon:\n", "            return random.randrange(self.action_size)\n", "        act_values = self.model.predict(state, verbose=0)\n", "        return np.argmax(act_values[0])\n", "    \n", "    def replay(self, batch_size):\n", "        if len(self.memory) < batch_size:\n", "            return\n", "        minibatch = random.sample(self.memory, batch_size)\n", "        states = np.array([sample[0] for sample in minibatch])\n", "        next_states = np.array([sample[3] for sample in minibatch])\n", "        \n", "        # Reshape states and next_states to (batch_size, state_size)\n", "        states = np.reshape(states, (batch_size, self.state_size))\n", "        next_states = np.reshape(next_states, (batch_size, self.state_size))\n", "        \n", "        targets = self.model.predict(states, verbose=0)\n", "        next_q_values = self.model.predict(next_states, verbose=0)\n", "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n", "            if done:\n", "                targets[i][action] = reward\n", "            else:\n", "                targets[i][action] = reward + self.gamma * np.amax(next_q_values[i])\n", "        self.model.fit(states, targets, epochs=1, verbose=0)\n", "        if self.epsilon > self.epsilon_min:\n", "            self.epsilon *= self.epsilon_decay"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initialize the agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["agent = DQNAgent(state_size, action_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Train the agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for e in range(episodes):\n", "    state = encoded_states[e % len(encoded_states)]  # Cycle through states\n", "    state = np.reshape(state, [1, state_size])  # Reshape to (1, state_size)\n", "    for step in range(len(encoded_actions[e % len(encoded_actions)])):\n", "        action = agent.act(state)\n", "        next_state = encoded_states[(e + 1) % len(encoded_states)]\n", "        next_state = np.reshape(next_state, [1, state_size])  # Reshape to (1, state_size)\n", "        reward = rewards[e % len(rewards)]\n", "        done = step == len(encoded_actions[e % len(encoded_actions)]) - 1\n", "        agent.remember(state, action, reward, next_state, done)\n", "        state = next_state\n", "        if done:\n", "            print(f\"Episode: {e}/{episodes}, Score: {reward}, Epsilon: {agent.epsilon}\")\n", "            break\n", "    if len(agent.memory) > batch_size:\n", "        agent.replay(batch_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Save the trained model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.save(\"dqn_model.h5\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Store rewards during training"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["episode_rewards = []"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Modify the training loop to store rewards"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for e in range(episodes):\n", "    episode_reward = 0\n", "    state = encoded_states[e % len(encoded_states)]\n", "    state = np.reshape(state, [1, state_size])\n", "    for step in range(len(encoded_actions[e % len(encoded_actions)])):\n", "        action = agent.act(state)\n", "        next_state = encoded_states[(e + 1) % len(encoded_states)]\n", "        next_state = np.reshape(next_state, [1, state_size])\n", "        reward = rewards[e % len(rewards)]\n", "        done = step == len(encoded_actions[e % len(encoded_actions)]) - 1\n", "        agent.remember(state, action, reward, next_state, done)\n", "        state = next_state\n", "        episode_reward += reward\n", "        if done:\n", "            break\n", "    episode_rewards.append(episode_reward)\n", "    if len(agent.memory) > batch_size:\n", "        agent.replay(batch_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot rewards over episodes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(episode_rewards)\n", "plt.xlabel(\"Episode\")\n", "plt.ylabel(\"Total Reward\")\n", "plt.title(\"Training Progress\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pip install matplotlib"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Store losses during training"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["losses = []"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Modify the replay method to store losses"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def replay(self, batch_size):\n", "    if len(self.memory) < batch_size:\n", "        return\n", "    minibatch = random.sample(self.memory, batch_size)\n", "    states = np.array([sample[0] for sample in minibatch])\n", "    next_states = np.array([sample[3] for sample in minibatch])\n", "    states = np.reshape(states, (batch_size, self.state_size))\n", "    next_states = np.reshape(next_states, (batch_size, self.state_size))\n", "    targets = self.model.predict(states, verbose=0)\n", "    next_q_values = self.model.predict(next_states, verbose=0)\n", "    for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n", "        if done:\n", "            targets[i][action] = reward\n", "        else:\n", "            targets[i][action] = reward + self.gamma * np.amax(next_q_values[i])\n", "    history = self.model.fit(states, targets, epochs=1, verbose=0)\n", "    losses.append(history.history['loss'][0])\n", "    if self.epsilon > self.epsilon_min:\n", "        self.epsilon *= self.epsilon_decay"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot losses over training steps"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(losses)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%% [markdown]<br>\n", "Training metrics<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import random\n", "from collections import deque, Counter\n", "import matplotlib.pyplot as plt  # Add this import\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense\n", "from tensorflow.keras.optimizers import Adam\n", "from tensorflow.keras.preprocessing.sequence import pad_sequences"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load your dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_df = pd.read_pickle(\"C:/Users/Rewan/Downloads/train_data.pickle\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Preprocess the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def preprocess_data(train_df):\n", "    states = train_df['start'].tolist()\n", "    actions = train_df['moves'].tolist()\n", "    rewards = train_df['score'].tolist()\n", "    \n", "    # Encode states (example: one-hot encoding)\n", "    def encode_state(sequence):\n", "        nucleotides = ['A', 'T', 'C', 'G', '-']\n", "        encoding = np.zeros((len(sequence), len(nucleotides)))\n", "        for i, char in enumerate(sequence):\n", "            if char in nucleotides:\n", "                encoding[i, nucleotides.index(char)] = 1\n", "        return encoding.flatten()\n", "    \n", "    # Pad sequences to a fixed length\n", "    max_length = max(len(seq) for seq in states)\n", "    encoded_states = [encode_state(seq) for seq in states]\n", "    encoded_states = pad_sequences(encoded_states, maxlen=max_length, padding='post', dtype='float32')\n", "    \n", "    # Encode actions (example: assign unique integers)\n", "    unique_actions = list(set([move for sublist in actions for move in sublist]))\n", "    action_to_index = {action: i for i, action in enumerate(unique_actions)}\n", "    encoded_actions = [[action_to_index.get(move, -1) for move in sublist] for sublist in actions]\n", "    \n", "    return encoded_states, encoded_actions, rewards"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Preprocess the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["encoded_states, encoded_actions, rewards = preprocess_data(train_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["DQN Hyperparameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["state_size = encoded_states.shape[1]\n", "action_size = len(set([action for sublist in encoded_actions for action in sublist]))\n", "batch_size = 32\n", "episodes = 1000\n", "memory = deque(maxlen=2000)\n", "gamma = 0.95\n", "epsilon = 1.0\n", "epsilon_min = 0.01\n", "epsilon_decay = 0.995\n", "learning_rate = 0.001"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Build the DQN model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(Dense(24, input_dim=state_size, activation='relu'))\n", "model.add(Dense(24, activation='relu'))\n", "model.add(Dense(action_size, activation='linear'))\n", "model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["DQN Agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DQNAgent:\n", "    def __init__(self, state_size, action_size):\n", "        self.state_size = state_size\n", "        self.action_size = action_size\n", "        self.memory = deque(maxlen=2000)\n", "        self.gamma = gamma\n", "        self.epsilon = epsilon\n", "        self.epsilon_min = epsilon_min\n", "        self.epsilon_decay = epsilon_decay\n", "        self.model = model\n", "    \n", "    def remember(self, state, action, reward, next_state, done):\n", "        self.memory.append((state, action, reward, next_state, done))\n", "    \n", "    def act(self, state):\n", "        if np.random.rand() <= self.epsilon:\n", "            return random.randrange(self.action_size)\n", "        act_values = self.model.predict(state, verbose=0)\n", "        return np.argmax(act_values[0])\n", "    \n", "    def replay(self, batch_size):\n", "        if len(self.memory) < batch_size:\n", "            return\n", "        minibatch = random.sample(self.memory, batch_size)\n", "        states = np.array([sample[0] for sample in minibatch])\n", "        next_states = np.array([sample[3] for sample in minibatch])\n", "        states = np.reshape(states, (batch_size, self.state_size))\n", "        next_states = np.reshape(next_states, (batch_size, self.state_size))\n", "        targets = self.model.predict(states, verbose=0)\n", "        next_q_values = self.model.predict(next_states, verbose=0)\n", "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n", "            if done:\n", "                targets[i][action] = reward\n", "            else:\n", "                targets[i][action] = reward + self.gamma * np.amax(next_q_values[i])\n", "        history = self.model.fit(states, targets, epochs=1, verbose=0)\n", "        losses.append(history.history['loss'][0])\n", "        if self.epsilon > self.epsilon_min:\n", "            self.epsilon *= self.epsilon_decay"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initialize the agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["agent = DQNAgent(state_size, action_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lists to store metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["episode_rewards = []\n", "losses = []\n", "epsilon_history = []\n", "actions_chosen = []\n", "q_values_history = []"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Train the agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for e in range(episodes):\n", "    episode_reward = 0\n", "    state = encoded_states[e % len(encoded_states)]\n", "    state = np.reshape(state, [1, state_size])\n", "    for step in range(len(encoded_actions[e % len(encoded_actions)])):\n", "        action = agent.act(state)\n", "        next_state = encoded_states[(e + 1) % len(encoded_states)]\n", "        next_state = np.reshape(next_state, [1, state_size])\n", "        reward = rewards[e % len(rewards)]\n", "        done = step == len(encoded_actions[e % len(encoded_actions)]) - 1\n", "        agent.remember(state, action, reward, next_state, done)\n", "        state = next_state\n", "        episode_reward += reward\n", "        actions_chosen.append(action)\n", "        if done:\n", "            break\n", "    episode_rewards.append(episode_reward)\n", "    epsilon_history.append(agent.epsilon)\n", "    q_values = model.predict(np.reshape(encoded_states[e % len(encoded_states)], [1, state_size]), verbose=0)[0]\n", "    q_values_history.append(q_values)\n", "    if len(agent.memory) > batch_size:\n", "        agent.replay(batch_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print and plot metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Average Reward per Episode:\", np.mean(episode_rewards))\n", "print(\"Cumulative Reward:\", np.sum(episode_rewards))\n", "print(\"Final Epsilon:\", agent.epsilon)\n", "print(\"Action Distribution:\", Counter(actions_chosen))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Average Reward per Episode"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 1)\n", "plt.plot(episode_rewards)\n", "plt.xlabel(\"Episode\")\n", "plt.ylabel(\"Average Reward\")\n", "plt.title(\"Average Reward per Episode\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cumulative Reward"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 2)\n", "plt.plot(np.cumsum(episode_rewards))\n", "plt.xlabel(\"Episode\")\n", "plt.ylabel(\"Cumulative Reward\")\n", "plt.title(\"Cumulative Reward Over Episodes\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Q-Value Convergence"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 3)\n", "plt.plot([q[0] for q in q_values_history])\n", "plt.xlabel(\"Episode\")\n", "plt.ylabel(\"Q-Value\")\n", "plt.title(\"Q-Value Convergence for Action 0\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Training Loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 4)\n", "plt.plot(losses)\n", "plt.xlabel(\"Training Step\")\n", "plt.ylabel(\"Loss\")\n", "plt.title(\"Training Loss Over Time\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Exploration Rate (Epsilon)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 5)\n", "plt.plot(epsilon_history)\n", "plt.xlabel(\"Episode\")\n", "plt.ylabel(\"Epsilon\")\n", "plt.title(\"Exploration Rate (Epsilon) Over Episodes\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Action Distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 6)\n", "action_counts = Counter(actions_chosen)\n", "plt.bar(action_counts.keys(), action_counts.values())\n", "plt.xlabel(\"Action\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Action Distribution\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%% [markdown]<br>\n", "Testing metrics<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import random\n", "from collections import deque, Counter\n", "import matplotlib.pyplot as plt  # Add this import\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense\n", "from tensorflow.keras.optimizers import Adam\n", "from tensorflow.keras.preprocessing.sequence import pad_sequences"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load your dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["test_df = pd.read_pickle(\"C:/Users/Rewan/Downloads/test_data.pickle\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Preprocess the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def preprocess_data(test_df):\n", "    states = train_df['start'].tolist()\n", "    actions = train_df['moves'].tolist()\n", "    rewards = train_df['score'].tolist()\n", "    \n", "    # Encode states (example: one-hot encoding)\n", "    def encode_state(sequence):\n", "        nucleotides = ['A', 'T', 'C', 'G', '-']\n", "        encoding = np.zeros((len(sequence), len(nucleotides)))\n", "        for i, char in enumerate(sequence):\n", "            if char in nucleotides:\n", "                encoding[i, nucleotides.index(char)] = 1\n", "        return encoding.flatten()\n", "    \n", "    # Pad sequences to a fixed length\n", "    max_length = max(len(seq) for seq in states)\n", "    encoded_states = [encode_state(seq) for seq in states]\n", "    encoded_states = pad_sequences(encoded_states, maxlen=max_length, padding='post', dtype='float32')\n", "    \n", "    # Encode actions (example: assign unique integers)\n", "    unique_actions = list(set([move for sublist in actions for move in sublist]))\n", "    action_to_index = {action: i for i, action in enumerate(unique_actions)}\n", "    encoded_actions = [[action_to_index.get(move, -1) for move in sublist] for sublist in actions]\n", "    \n", "    return encoded_states, encoded_actions, rewards"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Preprocess the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["encoded_states, encoded_actions, rewards = preprocess_data(test_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["DQN Hyperparameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["state_size = encoded_states.shape[1]\n", "action_size = len(set([action for sublist in encoded_actions for action in sublist]))\n", "batch_size = 32\n", "episodes = 1000\n", "memory = deque(maxlen=2000)\n", "gamma = 0.95\n", "epsilon = 1.0\n", "epsilon_min = 0.01\n", "epsilon_decay = 0.995\n", "learning_rate = 0.001"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Build the DQN model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(Dense(24, input_dim=state_size, activation='relu'))\n", "model.add(Dense(24, activation='relu'))\n", "model.add(Dense(action_size, activation='linear'))\n", "model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["DQN Agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DQNAgent:\n", "    def __init__(self, state_size, action_size):\n", "        self.state_size = state_size\n", "        self.action_size = action_size\n", "        self.memory = deque(maxlen=2000)\n", "        self.gamma = gamma\n", "        self.epsilon = epsilon\n", "        self.epsilon_min = epsilon_min\n", "        self.epsilon_decay = epsilon_decay\n", "        self.model = model\n", "    \n", "    def remember(self, state, action, reward, next_state, done):\n", "        self.memory.append((state, action, reward, next_state, done))\n", "    \n", "    def act(self, state):\n", "        if np.random.rand() <= self.epsilon:\n", "            return random.randrange(self.action_size)\n", "        act_values = self.model.predict(state, verbose=0)\n", "        return np.argmax(act_values[0])\n", "    \n", "    def replay(self, batch_size):\n", "        if len(self.memory) < batch_size:\n", "            return\n", "        minibatch = random.sample(self.memory, batch_size)\n", "        states = np.array([sample[0] for sample in minibatch])\n", "        next_states = np.array([sample[3] for sample in minibatch])\n", "        states = np.reshape(states, (batch_size, self.state_size))\n", "        next_states = np.reshape(next_states, (batch_size, self.state_size))\n", "        targets = self.model.predict(states, verbose=0)\n", "        next_q_values = self.model.predict(next_states, verbose=0)\n", "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n", "            if done:\n", "                targets[i][action] = reward\n", "            else:\n", "                targets[i][action] = reward + self.gamma * np.amax(next_q_values[i])\n", "        history = self.model.fit(states, targets, epochs=1, verbose=0)\n", "        losses.append(history.history['loss'][0])\n", "        if self.epsilon > self.epsilon_min:\n", "            self.epsilon *= self.epsilon_decay"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initialize the agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["agent = DQNAgent(state_size, action_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lists to store metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["episode_rewards = []\n", "losses = []\n", "epsilon_history = []\n", "actions_chosen = []\n", "q_values_history = []"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Train the agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for e in range(episodes):\n", "    episode_reward = 0\n", "    state = encoded_states[e % len(encoded_states)]\n", "    state = np.reshape(state, [1, state_size])\n", "    for step in range(len(encoded_actions[e % len(encoded_actions)])):\n", "        action = agent.act(state)\n", "        next_state = encoded_states[(e + 1) % len(encoded_states)]\n", "        next_state = np.reshape(next_state, [1, state_size])\n", "        reward = rewards[e % len(rewards)]\n", "        done = step == len(encoded_actions[e % len(encoded_actions)]) - 1\n", "        agent.remember(state, action, reward, next_state, done)\n", "        state = next_state\n", "        episode_reward += reward\n", "        actions_chosen.append(action)\n", "        if done:\n", "            break\n", "    episode_rewards.append(episode_reward)\n", "    epsilon_history.append(agent.epsilon)\n", "    q_values = model.predict(np.reshape(encoded_states[e % len(encoded_states)], [1, state_size]), verbose=0)[0]\n", "    q_values_history.append(q_values)\n", "    if len(agent.memory) > batch_size:\n", "        agent.replay(batch_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print and plot metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Average Reward per Episode:\", np.mean(episode_rewards))\n", "print(\"Cumulative Reward:\", np.sum(episode_rewards))\n", "print(\"Final Epsilon:\", agent.epsilon)\n", "print(\"Action Distribution:\", Counter(actions_chosen))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Average Reward per Episode"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 1)\n", "plt.plot(episode_rewards)\n", "plt.xlabel(\"Episode\")\n", "plt.ylabel(\"Average Reward\")\n", "plt.title(\"Average Reward per Episode\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cumulative Reward"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 2)\n", "plt.plot(np.cumsum(episode_rewards))\n", "plt.xlabel(\"Episode\")\n", "plt.ylabel(\"Cumulative Reward\")\n", "plt.title(\"Cumulative Reward Over Episodes\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Q-Value Convergence"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 3)\n", "plt.plot([q[0] for q in q_values_history])\n", "plt.xlabel(\"Episode\")\n", "plt.ylabel(\"Q-Value\")\n", "plt.title(\"Q-Value Convergence for Action 0\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Training Loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 4)\n", "plt.plot(losses)\n", "plt.xlabel(\"Training Step\")\n", "plt.ylabel(\"Loss\")\n", "plt.title(\"Training Loss Over Time\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Exploration Rate (Epsilon)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 5)\n", "plt.plot(epsilon_history)\n", "plt.xlabel(\"Episode\")\n", "plt.ylabel(\"Epsilon\")\n", "plt.title(\"Exploration Rate (Epsilon) Over Episodes\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Action Distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 3, 6)\n", "action_counts = Counter(actions_chosen)\n", "plt.bar(action_counts.keys(), action_counts.values())\n", "plt.xlabel(\"Action\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Action Distribution\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import random\n", "from collections import deque, Counter\n", "import matplotlib.pyplot as plt\n", "from tensorflow.keras.models import load_model\n", "from tensorflow.keras.preprocessing.sequence import pad_sequences"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load the test dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["test_df = pd.read_pickle(\"C:/Users/Rewan/Downloads/test_data.pickle\")  # Replace with your test dataset path"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Debug: Print column names to verify"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Columns in test_df:\", test_df.columns)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Preprocess the test data (same as training preprocessing)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def preprocess_data(test_df):\n", "    # Check if required columns exist\n", "    required_columns = ['start', 'moves', 'score']\n", "    for col in required_columns:\n", "        if col not in test_df.columns:\n", "            raise KeyError(f\"Column '{col}' not found in test_df. Available columns: {test_df.columns}\")\n", "    \n", "    states = test_df['start'].tolist()\n", "    actions = test_df['moves'].tolist()\n", "    rewards = test_df['score'].tolist()\n", "    \n", "    # Encode states (example: one-hot encoding)\n", "    def encode_state(sequence):\n", "        nucleotides = ['A', 'T', 'C', 'G', '-']\n", "        encoding = np.zeros((len(sequence), len(nucleotides)))\n", "        for i, char in enumerate(sequence):\n", "            if char in nucleotides:\n", "                encoding[i, nucleotides.index(char)] = 1\n", "        return encoding.flatten()\n", "    \n", "    # Pad sequences to a fixed length\n", "    max_length = max(len(seq) for seq in states)\n", "    encoded_states = [encode_state(seq) for seq in states]\n", "    encoded_states = pad_sequences(encoded_states, maxlen=max_length, padding='post', dtype='float32')\n", "    \n", "    # Encode actions (example: assign unique integers)\n", "    unique_actions = list(set([move for sublist in actions for move in sublist]))\n", "    action_to_index = {action: i for i, action in enumerate(unique_actions)}\n", "    encoded_actions = [[action_to_index.get(move, -1) for move in sublist] for sublist in actions]\n", "    \n", "    return encoded_states, encoded_actions, rewards"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Preprocess the test dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n", "    encoded_states, encoded_actions, rewards = preprocess_data(test_df)\n", "except KeyError as e:\n", "    print(f\"Error: {e}\")\n", "    exit()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load the trained model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n", "    model = load_model(r\"C:\\Users\\rewan\\Downloads\\dqn_model.h5\")\n", "    print(\"Model loaded successfully.\")\n", "except Exception as e:\n", "    print(f\"Error loading model: {e}\")\n", "    exit()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Test the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def test_model(model, encoded_states, encoded_actions, rewards):\n", "    total_reward = 0\n", "    predicted_actions = []\n", "    q_values_history = []\n", "    correct_predictions = 0  # Track correct predictions\n", "    \n", "    for i in range(len(encoded_states)):\n", "        state = encoded_states[i]\n", "        state = np.reshape(state, [1, -1])  # Reshape to (1, state_size)\n", "        \n", "        # Predict the action\n", "        action_values = model.predict(state, verbose=0)\n", "        action = np.argmax(action_values[0])\n", "        predicted_actions.append(action)\n", "        \n", "        # Compare with actual action (if available)\n", "        if encoded_actions[i]:  # Check if actual actions exist\n", "            actual_action = encoded_actions[i][0]  # Assuming single action per state\n", "            if action == actual_action:\n", "                correct_predictions += 1\n", "        \n", "        # Get the actual reward for this state-action pair\n", "        reward = rewards[i]\n", "        total_reward += reward\n", "        \n", "        # Store Q-values for analysis\n", "        q_values_history.append(action_values[0])\n", "        \n", "        print(f\"State {i + 1}: Predicted Action = {action}, Actual Reward = {reward}\")\n", "    \n", "    accuracy = correct_predictions / len(encoded_states) if len(encoded_states) > 0 else 0\n", "    print(f\"Total Reward: {total_reward}\")\n", "    print(f\"Prediction Accuracy: {accuracy * 100:.2f}%\")\n", "    return predicted_actions, q_values_history, total_reward"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run the test"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["predicted_actions, q_values_history, total_reward = test_model(model, encoded_states, encoded_actions, rewards)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Calculate evaluation metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["average_reward = total_reward / len(encoded_states)\n", "action_distribution = Counter(predicted_actions)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\nEvaluation Metrics:\")\n", "print(f\"Average Reward: {average_reward}\")\n", "print(f\"Total Reward: {total_reward}\")\n", "print(f\"Action Distribution: {action_distribution}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Q-Value Convergence for Action 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 2, 1)\n", "plt.plot([q[0] for q in q_values_history])\n", "plt.xlabel(\"State\")\n", "plt.ylabel(\"Q-Value\")\n", "plt.title(\"Q-Value Convergence for Action 0\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Action Distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 2, 2)\n", "action_counts = Counter(predicted_actions)\n", "plt.bar(action_counts.keys(), action_counts.values())\n", "plt.xlabel(\"Action\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Action Distribution\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cumulative Reward"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 2, 3)\n", "cumulative_rewards = np.cumsum(rewards)\n", "plt.plot(cumulative_rewards)\n", "plt.xlabel(\"State\")\n", "plt.ylabel(\"Cumulative Reward\")\n", "plt.title(\"Cumulative Reward Over States\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Reward Distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.subplot(2, 2, 4)\n", "plt.hist(rewards, bins=20)\n", "plt.xlabel(\"Reward\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Reward Distribution\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.tight_layout()\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}